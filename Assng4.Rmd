---
title: "Assng4"
author: "Saira Asif"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(rpart)
library(tree)
#install.packages("rpart.plot")
library(rpart.plot)
library(lars)
library(glmnet)
library(caret)
library(plotly)
library(randomForest)
library(ipred)
#install.packages("Metrics")
library(Metrics)
library(class)
library(e1071)
library(readr)
library(MASS)
install.packages("mlbench")
library(mlbench)
library(dplyr)
```

## DATA

```{r}
# reading in training and testing dataset
tumor_train <- read_csv(file.choose())
tumor_test <- read_csv(file.choose())

# No missing/NA values is any columns
apply(tumor_train, 2, function(x){sum(is.na(x))})

# No 0s in the predictor columns
apply(tumor_train, 2, function(x){sum(x==0)})

#convert status to factor
tumor_train$status<-as.factor(tumor_train$status)
tumor_test$status<-as.factor(tumor_test$status)
```

## elastic net (include Lasso and Ridge in your search grid) (Saira )

LASSO package glmnet requires the the response variable to be a vector and the set of predictor variables to be of the class data.matrix.

```{r}
#define response variable as vector 
status <- tumor_train$status
# dim is NULL as it is a vector, not a matrix
dim(status)

#define matrix of predictor variables
xs <- data.matrix(tumor_train[, -1])
dim(xs)
# Variance is different across predictors, so it has not been standardized, however glmnet automatically standardizes the variables
apply(xs, 2, var)

# ELASTIC NET

set.seed(12)

#efine the type of re-sampling as cross-validation using 10-fold method
enet <- trainControl(method = "cv", number = 10)

# Allows us to have more alpha values, from 0 to 1, to include both lasso and ridge regression in the analysis.
elasticGrid <- expand.grid(.alpha = seq(0, 1, length = 10), .lambda = seq(.5, 7.5, 1))
elasticGrid <- expand.grid(.alpha = seq(0, 1, length = 10), .lambda = seq(0, 1, length =10))

#cv.glmnet for computing penalized linear regression models.
def_elnet = train(status ~ ., data=tumor_train, method = "glmnet", trControl = enet, tuneGrid = elasticGrid)

# results for each combination of alpha and lambda
def_elnet$results

#lower alpha and lambda - better models
par(mfrow = c(1,2))
plot(def_elnet$results[ ,c(1,3)])
plot(def_elnet$results[ ,c(2,3)])

## highest possible Accuracy - best model
max_acc <- which.max(def_elnet$results[ ,3])
# The accuracy of this model is 94.38
def_elnet$results[max_acc,] 

# ridge regression seems the best for this dataset (alpha =0)
def_elnet$bestTune

# Compute the prediction error on the training set
train_enet <- glmnet(xs, status, type.measure = "class", family = "binomial", alpha = def_elnet$results[max_acc,1], lambda = def_elnet$results[max_acc,2])

train_enet

# Model coefficients
coef(def_elnet$finalModel, def_elnet$bestTune$lambda)

# Make predictions
predictions <- def_elnet %>% predict(tumor_test)

table(predictions, tumor_test[,1])

#perform k-fold cross-validation to find optimal lambda value that minimizes PE/MSE
cv_model <- cv.glmnet(xs, status, type.measure = "class", family = "binomial")
plot(cv_model)
coef(cv_model,s = c(cv_model$lambda.min,cv_model$lambda.1se))

prd.fl <- predict(cv_model, newx = data.matrix(tumor_test[,-1]), s = c(cv_model$lambda.min, cv_model$lambda.1se)) 

```


## KNN (Lisa)
Test for k=1 and k=5, to see the accuracy, then test again with quadratic effects 

```{r}

# Read the training and test sets from CSV files
trainset <- read.csv("trainset.csv")
testset <- read.csv("testset.csv")

# Set the seed for reproducibility
set.seed(123) 

# Separate the response variable from the predictors
train.X <- trainset[, 2:31]
train.Direction <- trainset[, 1]
test.X <- testset[, 2:31]
test.Direction <- testset[, 1]

# Set the seed for reproducibility
set.seed(123)

# Set status
train_status <- trainset$status
test_status <- testset$status

# Fit the model to knn() function with k=1
knn.pred <- knn(train = train.X, test = test.X, cl = train_status, k = 1)

#Create a confusion matrix
confusion_matrix <- table(knn.pred, test_status)

# Print the confusion matrix
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 2)))

# Calculate the sensitivity of the model 
sensitivity <- 57/(57+3)
print(paste("Sensitivity:", round(sensitivity,2)))

# Calculate the specificity of the model 
specificity <- 21/(21+4)
print(paste("Specificity:", round(specificity,2)))

# Fit the KNN model with k = 10 
knn.pred <- knn(train = train.X, test = test.X, cl = train_status, k = 10)

# Create a confusion matrix
confusion_matrix <- table(knn.pred, test_status)

# Print the confusion matrix
print(confusion_matrix)

# Calculate the accuracy of the model
accuracy_k10 <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy_k10 , 2)))

# Calculate the sensitivity of the model 
sensitivity_k10 <- 58/(58+2)
print(paste("Sensitivity:", round(sensitivity_k10,2)))

# Calculate the specificity of the model 
specificity <- 24/(24+1)
print(paste("Specificity:", round(specificity,2)))

# After increasing the k value from 1 to 10, the accuracy increased from 0.92 to 0.94. 

## Adding quadratic effects
# Quadratic dataset
train.Xsq <- train.X^2
test.Xsq <- test.X^2 

# Combine original squared predictors variable into new df
train.X_all <- cbind(train.X, train.Xsq)
test.X_all <- cbind(test.X, test.Xsq)

# Fit KNN model to new df 
knn.pred_all <- knn(train=train.X_all, test=test.X_all, cl= train_status, k=1)

# Create CM 
# k=1 
confusion_matrix_all_1 <- table(knn.pred_all, test_status)
accuracy_all_1 <- sum(diag(confusion_matrix_all_1)) / sum(confusion_matrix_all_1)
print(confusion_matrix_all_1)

# Calculate the sensitivity of the model 
sensitivity_all_1 <- 53/(53+7)
print(paste("Sensitivity:", round(sensitivity_all_1,2)))

# Calculate the specificity of the model 
specificity_all_1 <- 22/(22+3)
print(paste("Specificity:", round(specificity_all_1 ,2)))

# k=10 
knn.pred_all_10 <- knn(train = train.X_all, test = test.X_all, cl = train_status, k = 10)
confusion_matrix_all_10 <- table(knn.pred_all_10, test_status)
accuracy_all_10 <- sum(diag(confusion_matrix_all_10)) / sum(confusion_matrix_all_10)

# Print CM
print(confusion_matrix_all_10)
print(paste("Accuracy:", round(accuracy_all_10,2)))

# Calculate the sensitivity of the model 
sensitivity_all_10 <- 58/(58+2)
print(paste("Sensitivity:", round(sensitivity_all_10,2)))

# Calculate the specificity of the model 
specificity_all_10 <- 22/(22+3)
print(paste("Specificity:", round(specificity_all_10 ,2))) 

# Calculate feature importance based on reduction in accuracy when each feature is removed from the model. The higher the reduction in accuracy, the more important of the feature 
# k=1
importance_1 <- c()
for (i in 1:ncol(train.X_all)) {
  train.X_all_temp <- train.X_all[,-i]
  test.X_all_temp <- test.X_all[,-i]
  knn.pred_temp <- knn(train = train.X_all_temp, test = test.X_all_temp, cl = train_status, k = 1)
  confusion_matrix_temp <- table(knn.pred_temp, test_status)
  accuracy_temp <- sum(diag(confusion_matrix_temp)) / sum(confusion_matrix_temp)
  importance_1[i] <- accuracy_all_1 - accuracy_temp
}

# k=10
importance_10 <- c()
for (i in 1:ncol(train.X_all)) {
  train.X_all_temp <- train.X_all[,-i]
  test.X_all_temp <- test.X_all[,-i]
  knn.pred_temp <- knn(train = train.X_all_temp, test = test.X_all_temp, cl = train_status, k = 10)
  confusion_matrix_temp <- table(knn.pred_temp, test_status)
  accuracy_temp <- sum(diag(confusion_matrix_temp)) / sum(confusion_matrix_temp)
  importance_10[i] <- accuracy_all_10 - accuracy_temp
}

# Sort the feature importance values in descending order and select the top features 
# k=1
importance_order_1 <- order(importance_1, decreasing = TRUE)
top_features_1 <- importance_order_1[1:5]

# k=10
importance_order_10 <- order(importance_10, decreasing = TRUE)
top_features_10 <- importance_order_10[1:5]

# Print
cat("Top features for k=1:\n")
# Print the top features and their importance values
for (i in top_features_1) {
  cat(names(train.X_all)[i], "\n")

}
# for (i in top_features_1) {
#   cat(names(train.X_all)[i], ": ", round(importance_1[i], 4), "\n")
# }

# k=10
cat("\nTop features for k=10:\n")

for (i in top_features_10) {
  cat(names(train.X_all)[i], "\n")
# for (i in top_features_10) {
#   cat(names(train.X_all)[i], ": ", round(importance_10[i], 4), "\n")

}
```

The result using k=1 are pretty good with 92% of observations are correctly predicted. When improving the number of k to 10, the accuracy increased to 95%. 

## a fully grown classification/regression tree (CART) (Saira)

The training set is used to build a classification trees. The rpart function takes in a model with the status as the response and the 30 variables as predictors. The model is very accurate, with 90.59% of predicted classification being same as observed.

```{r}

# Fitting a classification tree to tumor dataset
class_tree <- rpart(status ~ ., data=tumor_train, method = "class")

# summary of classification tree
summary(class_tree)

# Figure # : Visualizing the unpruned classification tree consisting of relevant predictors to classify tumor status 
rpart.plot(class_tree)


# Checking the importance of key predictors to the classification tree
class_tree$variable.importance

# Checking model fit on testing dataset
pred_tree = predict(class_tree, tumor_test[2:31], type = "class")


# Confusion matrix of expected by predicted results as well as metric of Accuracy, Sensitivity and Specificity. 
confusionMatrix(table(pred_tree,tumor_test$status))

# Accuracy : 0.9059
# Sensitivity : 0.8833          
# Specificity : 0.9600 

```

We can also check if pruning the tree will improve results. Pruning selects the cp (complexity parameter) value associated with a shorter tree that minimizes the cross-validated error rate (xerror). 

```{r}
# Checking if pruning will improve results 
printcp(class_tree)

# Explicitly request the lowest complexity parameter (cp) value
bestcp <- class_tree$cptable[which.min(class_tree$cptable[,"xerror"]),"CP"]
pruned_tree <- prune(class_tree, cp = bestcp)
rpart.plot(pruned_tree)

# Pruned model has 9 important predictors, the unpruned tree used 11.
pruned_tree$variable.importance 


# Alternate specification 
pred_prune = predict(pruned_tree, tumor_test, type="class")

# Accuracy, Specificity and Sensitivity reuslts are almost the same as before
# No significant change is made when the tree is pruned
confusionMatrix(table(pred_prune,tumor_test$status))

```
There is no change in classification after pruning the tree. This suggests that pruning is not necessary for this tree


## support vector machine (Lisa)

```{r}
# Read the training and test sets from CSV files
trainset <- read.csv("trainset.csv")
testset <- read.csv("testset.csv")

# Select only the numeric columns from the training set
x <- trainset[,2:31]
y <- trainset$status

# Create a data frame with x and y columns
dat <- data.frame(x=x, y=as.factor(y))

# Train a support vector machine (SVM) with a linear kernel on the training data
svmfit <- svm(y~., data=dat, kernel="linear", cost =0.1, scale=FALSE)

# Tune the SVM hyperparameters using cross-validation on the training data
set.seed(1)
tune.out <- tune(svm, y ~ ., data=dat, kernel='linear', ranges=list(cost = c(0.001, 0.01, 0.1, 1, 3,5,10,100)))

# Print a summary of the tuning results
summary(tune.out)

# Extract the best performing SVM model from the tuning results
bestmod <- tune.out$best.model
summary(bestmod)

# Select only the numeric columns from the test set
xtest <- testset[,2:31]
ytest <- testset$status

# Create a data frame with xtest and ytest columns
testdat <- data.frame(x=xtest, y= as.factor(ytest))

# Use the best SVM model to predict the target variable for the test data
ypred <- predict(bestmod, testdat)

# Create a confusion matrix and calculate accuracy
conf_mat <- confusionMatrix(ypred, testdat$y)
accuracy <- conf_mat$overall["Accuracy"]

# Print the confusion matrix and accuracy
print(conf_mat$table)
cat("\nAccuracy:", round(accuracy, 2))

# Adding quadratic terms for each feature
x2 <- apply(x, 2, poly, degree=2, raw=TRUE)

# Create a new data frame with quadratic terms
dat2 <- data.frame(x2, y=as.factor(y)) 

# Train a support vector machine (SVM) with a linear kernel on the training data with quadratic terms
svmfit2 <- svm(y~., data=dat2, kernel="linear", cost=0.1, scale=FALSE)

# Tune the SVM hyperparameters using cross-validation on the training data with quadratic terms
set.seed(2)
tune.out2 <- tune(svm, y~., data=dat2, kernel="linear", ranges=list(cost = c(0.001, 0.01, 0.1, 1, 3,5,10,100)))

# Print a summary of the tuning results with quadratic terms
summary(tune.out2)

# Extract the best performing SVM model from the tuning results with quadratic terms
bestmod2 <- tune.out2$best.model
summary(bestmod2)

# Add quadratic terms for each feature in the test data
xtest2 <- apply(xtest, 2, poly, degree=2, raw=TRUE)
testdat2 <- data.frame(xtest2, y = as.factor(ytest))

# Use the best SVM model to predict the target variable for the test data with quadratic terms
ypred2 <- predict(bestmod2, testdat2)

# Create a confusion matrix and calculate accuracy with quadratic terms
conf_mat2 <- confusionMatrix(ypred2, testdat2$y)
accuracy2 <- conf_mat$overall["Accuracy"]

# Print the confusion matrix and accuracy with quadratic terms
print(conf_mat2$table)
cat("\nAccuracy:", round(accuracy2, 2))

# Calculate feature importance based on decrease in accuracy when each feature is removed
importance2 <- c()
for (i in 1:ncol(dat2[,2:31])) {
  dat2_temp <- dat2[,-i]
  svmfit2_temp <- svm(y ~ ., data = dat2_temp, kernel = "linear", cost = 0.1, scale = FALSE)
  ypred2_temp <- predict(svmfit2_temp, testdat2)
  conf_mat2_temp <- confusionMatrix(ypred2_temp, testdat2$y)
  accuracy2_temp <- conf_mat2_temp$overall["Accuracy"]
  importance2[i] <- accuracy2 - accuracy2_temp
}

# Rank features by importance and highlight top 5
ranked_importance2 <- order(importance2, decreasing = TRUE)
top_5_2 <- ranked_importance2[1:5]
cat("\nTop 5 most important features (including quadratic effects):\n")
cat(colnames(dat2)[top_5_2], sep = "\n")

```

## a bagged version of CART (Zee)
```{r}
##bagging regression trees with 150 bootstrap replications
set.seed(123)
bag<-bagging(status~., data=tumor_train, nbagg=150, coob=T)
bag
#Out-of-bag estimate of misclassification error:  0.0455

#most import variables for model
imp <- data.frame(var=names(tumor_train[,-1]), imp=varImp(bag))

#sort variable importance descending
VI_plot <- imp[order(imp$Overall, decreasing=TRUE),]

#visualize variable importance with horizontal bar plot
barplot(VI_plot$Overall,
        names.arg=rownames(VI_plot),
        horiz=TRUE,
        col='steelblue',
        xlab='Variable Importance')

bag_CART_pred<-predict(bag, newdata=tumor_test, type="class")

#confusionmatrix
confusionMatrix(table(bag_CART_pred,tumor_test$status))
#Accuracy=92.94%
#Sensitivity=93.33%
#Specificity=92%
```

## random forests. (Zee)
```{r}
#for reproducible results
set.seed(751)

#Creating random forest model
randomforest_model<-randomForest(status~., data=tumor_train)
randomforest_model
#random forest classification with 500 trees generated at each node, which splits into 5 smaller nodes

#visualization of most important variables in the model
varImpPlot(randomforest_model) 

#testing the model on unseen data
randomforest_test <- predict(randomforest_model, newdata = tumor_test, type= "class")

#Confusion matrix
confusionMatrix(table(randomforest_test,tumor_test$status)) 
###Accuracy= 94%
##Sensitivity= 95%
##Specificity= 92%  
```

## Final table 

```{r}
tab <- matrix( ncol=3, nrow=6,byrow=TRUE)
rownames(tab) <- c("Elastic Net", "KNN", "KNN_sq", "CART", "SVM", "SVM_sq", "Bagged CART", "Random Forest")
colnames(tab) <- c("Acc", "Specificity", "Sensitivity")

tab[1,] <-
tab[2,] <- c()
tab[3,] <- c(round(Acc*100, 2), round(SPES*100,2), round(SEN*100,2) )
tab[4,]
tab[5,] <- c(92.94,92, 93.33)
tab[6,] <- c(94, 92, 95)
tab
```

